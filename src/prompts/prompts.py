aten_to_cuda = """
Your task is to translate high-level operator descriptions into functional CUDA kernel code.

You will receive text containing:
- "Aggregated Operator Performance" (e.g., aten::mm, aten::sin) 
- "Detailed CUDA Kernel Breakdown" (e.g. volta_sgemm_128x64_nn, void at::native::vectorized_elementwise_kernel<4, at::native::sin_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::sin_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>))

Based on this information, write the corresponding CUDA __global__ kernel functions.

Critically Important Rules:

Your response MUST contain ONLY the raw CUDA C++ code with a main function.

For matrix multiplication such as aten::mm, provide a standard, non-tiled implementation.

For element-wise operations such as aten::sin, provide a standard grid-stride loop implementation.

Always print the final output variable to stdout

Printing should be done as typical python tensor format using a function like:

#include <iomanip> // for std::setprecision

// Print in PyTorch tensor format
std::cout << "tensor([" << std::endl;
for (int i = 0; i < 5; ++i) {
    std::cout << "        [";
    for (int j = 0; j < 5; ++j) {
        std::cout << std::fixed << std::setprecision(4)
                  << std::setw(8) << h_C_mm_out[i * MM_N + j];
        if (j < 4) std::cout << ", ";
    }
    if (i < 4) std::cout << "],";
    std::cout << std::endl;
}
std::cout << "        ...]," << std::endl;
std::cout << "       device='cuda:0')" << std::endl;

"""


aten_to_cuda_fixer = """
Your task is to fix a CUDA kernel code given an existing flawed CUDA kernel, correct high-level operator descriptions of original PyTorch, Error Message of why the CUDA broke.


High Level Operator will be formatted something like:
- "Aggregated Operator Performance" (e.g., aten::mm, aten::sin) 
- "Detailed CUDA Kernel Breakdown" (e.g. volta_sgemm_128x64_nn, void at::native::vectorized_elementwise_kernel<4, at::native::sin_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::sin_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>))

Based on this information, write an updated CUDA __global__ kernel function.

Critically Important Rules:

Your response MUST contain ONLY the raw CUDA C++ code with a main function.

For matrix multiplication such as aten::mm, provide a standard, non-tiled implementation.

For element-wise operations such as aten::sin, provide a standard grid-stride loop implementation.

Always print the final output variable to stdout


Printing should be done as typical python tensor format using a function like:

#include <iomanip> // for std::setprecision

// Print in PyTorch tensor format
std::cout << "tensor([" << std::endl;
for (int i = 0; i < 5; ++i) {
    std::cout << "        [";
    for (int j = 0; j < 5; ++j) {
        std::cout << std::fixed << std::setprecision(4)
                  << std::setw(8) << h_C_mm_out[i * MM_N + j];
        if (j < 4) std::cout << ", ";
    }
    if (i < 4) std::cout << "],";
    std::cout << std::endl;
}
std::cout << "        ...]," << std::endl;
std::cout << "       device='cuda:0')" << std::endl;
"""

def get_generation_sys_prompt(outputIR: str) -> str:
    """Retrieves System Prompt for generating initial kernels.

    Args:
        outputIR (str): Desired IR/Kernel to be generated by LLM

    Returns:
        str: System prompt for the IR/Kernel
    """

    if outputIR == "CUDA":
        return aten_to_cuda
    else:
        return ""

def get_fixer_sys_prompt(outputIR: str) -> str:
    """Retrieves System Prompt for fixing broken kernels.

    Args:
        outputIR (str): Desired IR/Kernel to be generated by LLM

    Returns:
        str: System prompt for the IR/Kernel
    """

    if outputIR == "CUDA":
        return aten_to_cuda_fixer
    else:
        return ""

def generate_fixer_prompt(kernel: str, error: str, msg: str,) -> str:
    """Generates a prompt used for an LLM to fix an existing kernels.

    Args:
        kernel (str): Previous version of the malformed/incorrect Kernel generated
        error (str): Custom error message to inform what the LLM did wrong 
        msg (str): Context the ORIGINAL LLM had to generate Kernel/IR

    Returns:
        str: Merged prompt for the LLM to fix the Kernel/IR
    """
    
    prompt = f"""
    [BROKEN KERNEL]
    {kernel}

    [ERROR/REASON IT NEEDS FIXING]
    {error}

    [ORIGINAL CONTEXT]
    {msg}
    """

    return prompt